{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = SparkSession.builder.master(\"local\") \\\n",
    "    .config('spark.sql.autoBroadcastJoinThreshold', 0) \\\n",
    "    .config('spark.jars.packages', 'ch.cern.sparkmeasure:spark-measure_2.12:0.23') \\\n",
    "    .config('spark.sparkmeasure.influxdbURL', 'http://sparkdashboard:8086') \\\n",
    "    .config('spark.extraListeners', 'ch.cern.sparkmeasure.InfluxDBSink') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасеты можно скачать из репозитория \n",
    "\n",
    "https://github.com/databricks/LearningSparkV2/tree/master/databricks-datasets/learning-spark-v2/flights\n",
    "\n",
    "https://github.com/databricks/LearningSparkV2/tree/master/databricks-datasets/learning-spark-v2/sf-fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes = spark.read \\\n",
    "    .option(\"header\", \"true\").option(\"delimiter\", \"\\t\").option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"../datasets/airport-codes-na.txt\") \\\n",
    "    .persist(pyspark.StorageLevel.DISK_ONLY)\n",
    "airport_delays = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"../datasets/departuredelays.csv\") \\\n",
    "    .persist(pyspark.StorageLevel.DISK_ONLY)\n",
    "\n",
    "fire_calls = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"../datasets/sf-fire-calls.csv\") \\\n",
    "    .persist(pyspark.StorageLevel.DISK_ONLY)\n",
    "fire_incidents = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"../datasets/sf-fire-incidents.csv\") \\\n",
    "    .persist(pyspark.StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример работы оптимизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('State = LA)\n",
      "+- Join Inner, (IATA#1644 = origin#1669)\n",
      "   :- Relation [date#1666,delay#1667,distance#1668,origin#1669,destination#1670] csv\n",
      "   +- Relation [City#1641,State#1642,Country#1643,IATA#1644] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "date: int, delay: int, distance: int, origin: string, destination: string, City: string, State: string, Country: string, IATA: string\n",
      "Filter (State#1642 = LA)\n",
      "+- Join Inner, (IATA#1644 = origin#1669)\n",
      "   :- Relation [date#1666,delay#1667,distance#1668,origin#1669,destination#1670] csv\n",
      "   +- Relation [City#1641,State#1642,Country#1643,IATA#1644] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (IATA#1644 = origin#1669)\n",
      ":- Filter isnotnull(origin#1669)\n",
      ":  +- InMemoryRelation [date#1666, delay#1667, distance#1668, origin#1669, destination#1670], StorageLevel(disk, 1 replicas)\n",
      ":        +- FileScan csv [date#62,delay#63,distance#64,origin#65,destination#66] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/datasets/departuredelays.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<date:int,delay:int,distance:int,origin:string,destination:string>\n",
      "+- Filter ((isnotnull(State#1642) AND (State#1642 = LA)) AND isnotnull(IATA#1644))\n",
      "   +- InMemoryRelation [City#1641, State#1642, Country#1643, IATA#1644], StorageLevel(disk, 1 replicas)\n",
      "         +- FileScan csv [City#17,State#18,Country#19,IATA#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/datasets/airport-codes-na.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<City:string,State:string,Country:string,IATA:string>\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [origin#1669], [IATA#1644], Inner\n",
      "   :- Sort [origin#1669 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(origin#1669, 200), ENSURE_REQUIREMENTS, [plan_id=768]\n",
      "   :     +- Filter isnotnull(origin#1669)\n",
      "   :        +- InMemoryTableScan [date#1666, delay#1667, distance#1668, origin#1669, destination#1670], [isnotnull(origin#1669)]\n",
      "   :              +- InMemoryRelation [date#1666, delay#1667, distance#1668, origin#1669, destination#1670], StorageLevel(disk, 1 replicas)\n",
      "   :                    +- FileScan csv [date#62,delay#63,distance#64,origin#65,destination#66] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/datasets/departuredelays.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<date:int,delay:int,distance:int,origin:string,destination:string>\n",
      "   +- Sort [IATA#1644 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(IATA#1644, 200), ENSURE_REQUIREMENTS, [plan_id=769]\n",
      "         +- Filter ((isnotnull(State#1642) AND (State#1642 = LA)) AND isnotnull(IATA#1644))\n",
      "            +- InMemoryTableScan [City#1641, State#1642, Country#1643, IATA#1644], [isnotnull(State#1642), (State#1642 = LA), isnotnull(IATA#1644)]\n",
      "                  +- InMemoryRelation [City#1641, State#1642, Country#1643, IATA#1644], StorageLevel(disk, 1 replicas)\n",
      "                        +- FileScan csv [City#17,State#18,Country#19,IATA#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/datasets/airport-codes-na.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<City:string,State:string,Country:string,IATA:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_delays \\\n",
    "    .join(airport_codes, airport_codes['IATA'] == airport_delays['origin']) \\\n",
    "    .filter(col('State') == 'LA') \\\n",
    "    .explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Солёный\" JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|    fruit|items_count|\n",
      "+---------+-----------+\n",
      "|    Apple|         20|\n",
      "|    Apple|         20|\n",
      "|    Apple|         20|\n",
      "|    Apple|         20|\n",
      "|    Apple|         20|\n",
      "|    Apple|         20|\n",
      "|    Apple|         20|\n",
      "|    Apple|         20|\n",
      "|    Apple|         20|\n",
      "|    Apple|         20|\n",
      "|   Orange|         10|\n",
      "|Pineapple|         30|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fruit_stats_columns = [\"fruit\", \"items_count\"]\n",
    "fruit_stats_data = [(\"Apple\", \"20\")] * 10 + [(\"Orange\", \"10\"), (\"Pineapple\", \"30\")]\n",
    "\n",
    "fruit_stats_data_rdd = spark.sparkContext.parallelize(fruit_stats_data)\n",
    "\n",
    "fruit_stats_data_df = fruit_stats_data_rdd.toDF(fruit_stats_columns)\n",
    "\n",
    "fruit_stats_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|    fruit| color|\n",
      "+---------+------+\n",
      "|    Apple|   red|\n",
      "|   Orange|orange|\n",
      "|Pineapple|yellow|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fruit_color_columns = [\"fruit\", \"color\"]\n",
    "fruit_color_data = [(\"Apple\", \"red\"), (\"Orange\", \"orange\"), (\"Pineapple\", \"yellow\")]\n",
    "\n",
    "fruit_color_data_rdd = spark.sparkContext.parallelize(fruit_color_data)\n",
    "\n",
    "fruit_color_data_df = fruit_color_data_rdd.toDF(fruit_color_columns)\n",
    "\n",
    "fruit_color_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|partition|partition_size|\n",
      "+---------+--------------+\n",
      "|Pineapple|             1|\n",
      "|   Orange|             1|\n",
      "|    Apple|            10|\n",
      "+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fruit_stats_data_df.groupBy(col('fruit').alias('partition')).agg(count(col('fruit')).alias('partition_size')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----+\n",
      "|    fruit|items_count|salt|\n",
      "+---------+-----------+----+\n",
      "|    Apple|         20|   1|\n",
      "|    Apple|         20|   0|\n",
      "|    Apple|         20|   2|\n",
      "|    Apple|         20|   0|\n",
      "|    Apple|         20|   0|\n",
      "|    Apple|         20|   2|\n",
      "|    Apple|         20|   0|\n",
      "|    Apple|         20|   2|\n",
      "|    Apple|         20|   1|\n",
      "|    Apple|         20|   1|\n",
      "|   Orange|         10|   2|\n",
      "|Pineapple|         30|   1|\n",
      "+---------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salted_df = fruit_stats_data_df.withColumn('salt', (rand() * 3).cast('int'))\n",
    "salted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "| partition|partition_size|\n",
      "+----------+--------------+\n",
      "|    Apple1|             3|\n",
      "|Pineapple1|             1|\n",
      "|    Apple2|             3|\n",
      "|    Apple0|             4|\n",
      "|   Orange2|             1|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salted_df.groupBy((concat(col('fruit'), col('salt'))).alias('partition')) \\\n",
    "                            .agg(count(col('fruit')).alias('partition_size')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----+\n",
      "|    fruit| color|salt|\n",
      "+---------+------+----+\n",
      "|    Apple|   red|   0|\n",
      "|    Apple|   red|   1|\n",
      "|    Apple|   red|   2|\n",
      "|   Orange|orange|   0|\n",
      "|   Orange|orange|   1|\n",
      "|   Orange|orange|   2|\n",
      "|Pineapple|yellow|   0|\n",
      "|Pineapple|yellow|   1|\n",
      "|Pineapple|yellow|   2|\n",
      "+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enriched_df = fruit_color_data_df \\\n",
    "    .select(col('fruit'), col('color'), explode(array([lit(i) for i in [0, 1, 2]])).alias('salt'))\n",
    "enriched_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------+\n",
      "|    fruit|items_count| color|\n",
      "+---------+-----------+------+\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|   Orange|         10|orange|\n",
      "|Pineapple|         30|yellow|\n",
      "+---------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = salted_df.join(enriched_df, ['fruit', 'salt'])\n",
    "joined_df.drop('salt').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------+\n",
      "|    fruit|items_count| color|\n",
      "+---------+-----------+------+\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|    Apple|         20|   red|\n",
      "|   Orange|         10|orange|\n",
      "|Pineapple|         30|yellow|\n",
      "+---------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fruit_stats_data_df.join(fruit_color_data_df, 'fruit').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример использования broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+----------+-----+-------+----+\n",
      "|   date|delay|distance|origin|destination|      City|State|Country|IATA|\n",
      "+-------+-----+--------+------+-----------+----------+-----+-------+----+\n",
      "|1011030|   63|     247|   AEX|        DFW|Alexandria|   LA|    USA| AEX|\n",
      "|1011838|  -14|     165|   AEX|        IAH|Alexandria|   LA|    USA| AEX|\n",
      "|1011204|   -8|     165|   AEX|        IAH|Alexandria|   LA|    USA| AEX|\n",
      "|1011710|   18|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "|1011115|   -3|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "|1021030|   -5|     247|   AEX|        DFW|Alexandria|   LA|    USA| AEX|\n",
      "|1020537|   -7|     165|   AEX|        IAH|Alexandria|   LA|    USA| AEX|\n",
      "|1021851|   -3|     165|   AEX|        IAH|Alexandria|   LA|    USA| AEX|\n",
      "|1021204|   21|     165|   AEX|        IAH|Alexandria|   LA|    USA| AEX|\n",
      "|1020600|   -6|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "|1021710|  115|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "|1021116|    9|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "|1021408|   52|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "|1031030|   56|     247|   AEX|        DFW|Alexandria|   LA|    USA| AEX|\n",
      "|1030537|    0|     165|   AEX|        IAH|Alexandria|   LA|    USA| AEX|\n",
      "|1031838|    0|     165|   AEX|        IAH|Alexandria|   LA|    USA| AEX|\n",
      "|1031204|  -14|     165|   AEX|        IAH|Alexandria|   LA|    USA| AEX|\n",
      "|1030600|    0|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "|1031710|   -5|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "|1031116|    7|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "+-------+-----+--------+------+-----------+----------+-----+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_delays \\\n",
    "    .join(airport_codes, airport_codes['IATA'] == airport_delays['origin']) \\\n",
    "    .filter(col('State') == 'LA') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+----------+-----+-------+----+\n",
      "|   date|delay|distance|origin|destination|      City|State|Country|IATA|\n",
      "+-------+-----+--------+------+-----------+----------+-----+-------+----+\n",
      "|1011030|   63|     247|   AEX|        DFW|Alexandria|   LA|    USA| AEX|\n",
      "|1011838|  -14|     165|   AEX|        IAH|Alexandria|   LA|    USA| AEX|\n",
      "|1011204|   -8|     165|   AEX|        IAH|Alexandria|   LA|    USA| AEX|\n",
      "|1011710|   18|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "|1011115|   -3|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "|1021030|   -5|     247|   AEX|        DFW|Alexandria|   LA|    USA| AEX|\n",
      "|1020537|   -7|     165|   AEX|        IAH|Alexandria|   LA|    USA| AEX|\n",
      "|1021851|   -3|     165|   AEX|        IAH|Alexandria|   LA|    USA| AEX|\n",
      "|1021204|   21|     165|   AEX|        IAH|Alexandria|   LA|    USA| AEX|\n",
      "|1020600|   -6|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "|1021710|  115|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "|1021116|    9|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "|1021408|   52|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "|1031030|   56|     247|   AEX|        DFW|Alexandria|   LA|    USA| AEX|\n",
      "|1030537|    0|     165|   AEX|        IAH|Alexandria|   LA|    USA| AEX|\n",
      "|1031838|    0|     165|   AEX|        IAH|Alexandria|   LA|    USA| AEX|\n",
      "|1031204|  -14|     165|   AEX|        IAH|Alexandria|   LA|    USA| AEX|\n",
      "|1030600|    0|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "|1031710|   -5|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "|1031116|    7|     435|   AEX|        ATL|Alexandria|   LA|    USA| AEX|\n",
      "+-------+-----+--------+------+-----------+----------+-----+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# С броадкастом\n",
    "\n",
    "bc_airport_codes = broadcast(airport_codes)\n",
    "\n",
    "airport_delays \\\n",
    "    .join(bc_airport_codes, airport_codes['IATA'] == airport_delays['origin']) \\\n",
    "    .filter(col('State') == 'LA') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример использования фильтра Блума"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_incidents.select(col('Incident Number')).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_calls.select(col('IncidentNumber')).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_calls \\\n",
    "    .join(fire_incidents, fire_incidents['Incident Number'] == fire_calls['IncidentNumber']) \\\n",
    "    .select(col('IncidentNumber')).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Без фильтра\n",
    "\n",
    "fire_calls \\\n",
    "    .join(fire_incidents, fire_incidents['Incident Number'] == fire_calls['IncidentNumber']) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir mmh3 bitarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import mmh3\n",
    "from bitarray import bitarray\n",
    "  \n",
    "  \n",
    "class BloomFilter(object):\n",
    "  \n",
    "    '''\n",
    "    Class for Bloom filter, using murmur3 hash function\n",
    "    '''\n",
    "  \n",
    "    def __init__(self, items_count, fp_prob):\n",
    "        '''\n",
    "        items_count : int\n",
    "            Number of items expected to be stored in bloom filter\n",
    "        fp_prob : float\n",
    "            False Positive probability in decimal\n",
    "        '''\n",
    "        self.items_count = items_count\n",
    "        \n",
    "        # False possible probability in decimal\n",
    "        self.fp_prob = fp_prob\n",
    "  \n",
    "        # Size of bit array to use\n",
    "        self.size = self.get_size(items_count, fp_prob)\n",
    "  \n",
    "        # number of hash functions to use\n",
    "        self.hash_count = self.get_hash_count(self.size, items_count)\n",
    "  \n",
    "        # Bit array of given size\n",
    "        self.bit_array = bitarray(self.size)\n",
    "  \n",
    "        # initialize all bits as 0\n",
    "        self.bit_array.setall(0)\n",
    "  \n",
    "    def add(self, item):\n",
    "        '''\n",
    "        Add an item in the filter\n",
    "        '''\n",
    "        digests = []\n",
    "        for i in range(self.hash_count):\n",
    "  \n",
    "            # create digest for given item.\n",
    "            # i work as seed to mmh3.hash() function\n",
    "            # With different seed, digest created is different\n",
    "            digest = mmh3.hash(item, i) % self.size\n",
    "            digests.append(digest)\n",
    "  \n",
    "            # set the bit True in bit_array\n",
    "            self.bit_array[digest] = True\n",
    "        \n",
    "    def union(self, other):\n",
    "        \"\"\" Calculates the union of the two underlying bitarrays and returns\n",
    "        a new bloom filter object.\"\"\"\n",
    "        new_bloom = self.copy()\n",
    "        new_bloom.bit_array = new_bloom.bit_array | other.bit_array\n",
    "        return new_bloom\n",
    "  \n",
    "    def check(self, item):\n",
    "        '''\n",
    "        Check for existence of an item in filter\n",
    "        '''\n",
    "        for i in range(self.hash_count):\n",
    "            digest = mmh3.hash(item, i) % self.size\n",
    "            if self.bit_array[digest] == False:\n",
    "  \n",
    "                # if any of bit is False then,its not present\n",
    "                # in filter\n",
    "                # else there is probability that it exist\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def copy(self):\n",
    "        \"\"\"Return a copy of this bloom filter.\n",
    "        \"\"\"\n",
    "        new_filter = BloomFilter(self.items_count, self.fp_prob)\n",
    "        new_filter.bit_array = self.bit_array.copy()\n",
    "        return new_filter\n",
    "    \n",
    "    def set_bit_array(self, bit_array):\n",
    "        self.bit_array = bit_array\n",
    "  \n",
    "    @classmethod\n",
    "    def get_size(self, n, p):\n",
    "        '''\n",
    "        Return the size of bit array(m) to used using\n",
    "        following formula\n",
    "        m = -(n * lg(p)) / (lg(2)^2)\n",
    "        n : int\n",
    "            number of items expected to be stored in filter\n",
    "        p : float\n",
    "            False Positive probability in decimal\n",
    "        '''\n",
    "        m = -(n * math.log(p))/(math.log(2)**2)\n",
    "        return int(m)\n",
    "  \n",
    "    @classmethod\n",
    "    def get_hash_count(self, m, n):\n",
    "        '''\n",
    "        Return the hash function(k) to be used using\n",
    "        following formula\n",
    "        k = (m/n) * lg(2)\n",
    "  \n",
    "        m : int\n",
    "            size of bit array\n",
    "        n : int\n",
    "            number of items expected to be stored in filter\n",
    "        '''\n",
    "        k = (m/n) * math.log(2)\n",
    "        return int(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "  \n",
    "n = 20 #no of items to add\n",
    "p = 0.05 #false positive probability\n",
    "  \n",
    "bloomf1 = BloomFilter(n,p)\n",
    "bloomf2 = BloomFilter(n,p)\n",
    "  \n",
    "# words to be added\n",
    "word_present1 = ['abound','abounds','abundance','abundant','accessible',\n",
    "                'bloom','blossom','bolster','bonny','bonus','bonuses']\n",
    "  \n",
    "# words to be added\n",
    "word_present2 = ['coherent','cohesive','colorful','comely','comfort',\n",
    "                'gems','generosity','generous','generously','genial']\n",
    "  \n",
    "# word not added\n",
    "word_absent = ['bluff','cheater','hate','war','humanity',\n",
    "               'racism','hurt','nuke','gloomy','facebook',\n",
    "               'geeksforgeeks','twitter']\n",
    "  \n",
    "for item in word_present1:\n",
    "    bloomf1.add(item)\n",
    "  \n",
    "for item in word_present:\n",
    "    bloomf2.add(item)\n",
    "    \n",
    "bloomf = bloomf1.union(bloomf2)\n",
    "  \n",
    "shuffle(word_present1)\n",
    "shuffle(word_present2)\n",
    "shuffle(word_absent)\n",
    "  \n",
    "test_words = word_present1 + word_present2 + word_absent\n",
    "shuffle(test_words)\n",
    "for word in test_words:\n",
    "    if bloomf.check(word):\n",
    "        if word in word_absent:\n",
    "            print(\"'{}' is a false positive!\".format(word))\n",
    "        else:\n",
    "            print(\"'{}' is probably present!\".format(word))\n",
    "    else:\n",
    "        print(\"'{}' is definitely not present!\".format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем фильтр\n",
    "\n",
    "filterSize = 31771\n",
    "prob = 0.05\n",
    "\n",
    "def fill_bloom_filter(bf, items):\n",
    "    for i in items:\n",
    "        bf.add(str(i[0]))\n",
    "    return bf\n",
    "\n",
    "bloom_filter = BloomFilter(filterSize, prob)\n",
    "\n",
    "general_bit_array = fire_incidents.select(col('Incident Number')).rdd \\\n",
    "    .mapPartitions(lambda p: [fill_bloom_filter(BloomFilter(filterSize, prob), p).bit_array]) \\\n",
    "    .reduce(lambda a, b: a.bit_array | b.bit_array)\n",
    "\n",
    "bloom_filter.set_bit_array(general_bit_array)\n",
    "\n",
    "maybe_in_bf = udf(lambda incident_number: bloom_filter.check(str(incident_number)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем фильтр\n",
    "\n",
    "fire_calls \\\n",
    "    .filter(maybe_in_bf(col('IncidentNumber')) == True) \\\n",
    "    .join(fire_incidents, fire_incidents['Incident Number'] == fire_calls['IncidentNumber']) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Без бакетинга\n",
    "\n",
    "fire_calls \\\n",
    "    .join(fire_incidents, fire_incidents['Incident Number'] == fire_calls['IncidentNumber']) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_calls.write \\\n",
    "    .bucketBy(16, 'IncidentNumber') \\\n",
    "    .saveAsTable('fire_calls_bucketed', format='csv', mode='overwrite')\n",
    "    \n",
    "fire_incidents.write \\\n",
    "    .bucketBy(16, 'Incident Number') \\\n",
    "    .saveAsTable('fire_incidents_bucketed', format='csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# С бакетингом\n",
    "\n",
    "fire_calls_bucketed = spark.table('fire_calls_bucketed')\n",
    "fire_incidents_bucketed = spark.table('fire_incidents_bucketed')\n",
    "\n",
    "fire_calls_bucketed \\\n",
    "    .join(fire_incidents_bucketed, fire_incidents_bucketed['Incident Number'] == fire_calls_bucketed['IncidentNumber']) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bc_airport_codes == None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_airport_codes.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes.select(col('State')).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_delays.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_incidents.select(col('Incident Number')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_calls.select(col('IncidentNumber')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_incidents \\\n",
    "    .join(fire_calls, fire_incidents['Incident Number'] == fire_calls['IncidentNumber']) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
